{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Construya una función que cargue todos los datos de entrenamiento y pruebas del problema generando\n",
    "como salida: (i) dos matrices $X_{tr}$, $Y_{tr}$, correspondientes a las imágenes y etiquetas de entrenamiento,\n",
    "(ii) dos matrices $X_t$, $Y_t$, correspondientes a las imágenes y etiquetas de pruebas, y finalmente (iii) dos\n",
    "matrices $X_v$, $Y_v$, correspondientes a imágenes y etiquetas que se usarán como conjunto de validación, es\n",
    "decir para tomar decisiones de diseño acerca del modelo. Este último conjunto debe ser extraído desde\n",
    "el conjunto de entrenamiento original y no debe superar las 7000 imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "def load_data():\n",
    "    train = pd.read_csv('data/sign_mnist_train.csv')\n",
    "    test = pd.read_csv('data/sign_mnist_test.csv')\n",
    "    (train_set, validation_set) = train_test_split(train, test_size=7000, random_state=8500)\n",
    "    \n",
    "    y_tr = train_set['label']\n",
    "    x_tr = train_set.iloc[:, 1:]\n",
    "    \n",
    "    y_t = test['label']\n",
    "    x_t = test.iloc[:, 1:]\n",
    "    \n",
    "    y_v = validation_set['label']\n",
    "    x_v = validation_set.iloc[:, 1:]\n",
    "\n",
    "    return(x_tr,x_v,x_t,y_tr,y_v,y_t)\n",
    "x_tr, x_v, x_t, y_tr, y_v , y_t = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Construya una función que escale apropiadamente las imágenes antes de trabajar. Experimente sólo\n",
    "escalando los datos de acuerdo a la intensidad máxima de pixel (i.e., dividiendo por 255) y luego\n",
    "centrando y escalándolos como en actividades anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_by_max_pixel(x_set):\n",
    "    return x_set/255\n",
    "\n",
    "\n",
    "# Finish this one!\n",
    "def center_and_scale(x_set):\n",
    "    return x_set\n",
    "\n",
    "x_tr_scaled = scale_by_max_pixel(x_tr)\n",
    "x_t_scaled = scale_by_max_pixel(x_t)\n",
    "x_v_scaled = scale_by_max_pixel(x_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Diseñe, entrene y evalúe una red neuronal para el problema partir de la representaci´on original de las\n",
    "im´agenes. Experimente con distintas arquitecturas, pre-procesamientos y m´etodos de entrenamiento, midiendo el error de clasificaci´on sobre el conjunto de validaci´on. En base a esta ´ultima medida de\n",
    "desempe˜no, decida qu´e modelo, de entre todos los evaluados, medir´a finalmente en el conjunto de test.\n",
    "Reporte y discuta los resultados obtenidos. Se espera que logre obtener un error de pruebas menor o\n",
    "igual a 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20455 samples, validate on 7000 samples\n",
      "Epoch 1/100\n",
      "20455/20455 [==============================] - 1s 58us/step - loss: 3.0911 - acc: 0.0839 - val_loss: 3.2702 - val_acc: 0.0644\n",
      "Epoch 2/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 2.6169 - acc: 0.1588 - val_loss: 2.6295 - val_acc: 0.1634\n",
      "Epoch 3/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 2.4001 - acc: 0.2069 - val_loss: 2.1223 - val_acc: 0.2790\n",
      "Epoch 4/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 2.2432 - acc: 0.2472 - val_loss: 2.1688 - val_acc: 0.2490\n",
      "Epoch 5/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 2.1558 - acc: 0.2666 - val_loss: 2.6278 - val_acc: 0.2286\n",
      "Epoch 6/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 2.0971 - acc: 0.2856 - val_loss: 2.3579 - val_acc: 0.2613\n",
      "Epoch 7/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 2.0405 - acc: 0.2947 - val_loss: 2.4723 - val_acc: 0.2137\n",
      "Epoch 8/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.9906 - acc: 0.3121 - val_loss: 2.0144 - val_acc: 0.2639\n",
      "Epoch 9/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.9554 - acc: 0.3196 - val_loss: 1.9065 - val_acc: 0.3079\n",
      "Epoch 10/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.9073 - acc: 0.3314 - val_loss: 2.0297 - val_acc: 0.2916\n",
      "Epoch 11/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.8865 - acc: 0.3370 - val_loss: 1.6745 - val_acc: 0.4101\n",
      "Epoch 12/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.8554 - acc: 0.3414 - val_loss: 1.8310 - val_acc: 0.3359\n",
      "Epoch 13/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.8258 - acc: 0.3503 - val_loss: 1.6980 - val_acc: 0.3903\n",
      "Epoch 14/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.8040 - acc: 0.3558 - val_loss: 1.8692 - val_acc: 0.3379\n",
      "Epoch 15/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.7764 - acc: 0.3732 - val_loss: 1.8237 - val_acc: 0.3329\n",
      "Epoch 16/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.7711 - acc: 0.3714 - val_loss: 1.8552 - val_acc: 0.3094\n",
      "Epoch 17/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.7475 - acc: 0.3742 - val_loss: 1.6426 - val_acc: 0.3886\n",
      "Epoch 18/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 1.7238 - acc: 0.3855 - val_loss: 2.9823 - val_acc: 0.1856\n",
      "Epoch 19/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.7030 - acc: 0.3898 - val_loss: 1.6941 - val_acc: 0.3651\n",
      "Epoch 20/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.6976 - acc: 0.3910 - val_loss: 2.0417 - val_acc: 0.3031\n",
      "Epoch 21/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.6841 - acc: 0.3931 - val_loss: 1.5088 - val_acc: 0.4414\n",
      "Epoch 22/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 1.6685 - acc: 0.4005 - val_loss: 1.5945 - val_acc: 0.4099\n",
      "Epoch 23/100\n",
      "20455/20455 [==============================] - 1s 44us/step - loss: 1.6529 - acc: 0.4083 - val_loss: 1.5748 - val_acc: 0.4236\n",
      "Epoch 24/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.6485 - acc: 0.4126 - val_loss: 1.6170 - val_acc: 0.4211\n",
      "Epoch 25/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.6280 - acc: 0.4110 - val_loss: 2.5698 - val_acc: 0.2437\n",
      "Epoch 26/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.6145 - acc: 0.4174 - val_loss: 1.5881 - val_acc: 0.4371\n",
      "Epoch 27/100\n",
      "20455/20455 [==============================] - 1s 44us/step - loss: 1.6053 - acc: 0.4173 - val_loss: 1.9085 - val_acc: 0.3820\n",
      "Epoch 28/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 1.6023 - acc: 0.4304 - val_loss: 1.4800 - val_acc: 0.4594\n",
      "Epoch 29/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.5875 - acc: 0.4235 - val_loss: 1.5135 - val_acc: 0.4207\n",
      "Epoch 30/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.5831 - acc: 0.4321 - val_loss: 1.6472 - val_acc: 0.4023\n",
      "Epoch 31/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.5578 - acc: 0.4404 - val_loss: 1.4442 - val_acc: 0.4754\n",
      "Epoch 32/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.5564 - acc: 0.4400 - val_loss: 1.7892 - val_acc: 0.4160\n",
      "Epoch 33/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.5399 - acc: 0.4467 - val_loss: 1.3675 - val_acc: 0.4860\n",
      "Epoch 34/100\n",
      "20455/20455 [==============================] - 1s 45us/step - loss: 1.5455 - acc: 0.4434 - val_loss: 1.4782 - val_acc: 0.4501\n",
      "Epoch 35/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 1.5409 - acc: 0.4477 - val_loss: 1.7361 - val_acc: 0.3786\n",
      "Epoch 36/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.5230 - acc: 0.4537 - val_loss: 1.3533 - val_acc: 0.4784\n",
      "Epoch 37/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.5123 - acc: 0.4571 - val_loss: 1.3954 - val_acc: 0.4734\n",
      "Epoch 38/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.5170 - acc: 0.4547 - val_loss: 1.5218 - val_acc: 0.4544\n",
      "Epoch 39/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.4885 - acc: 0.4593 - val_loss: 1.4004 - val_acc: 0.4831\n",
      "Epoch 40/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.4943 - acc: 0.4665 - val_loss: 1.3790 - val_acc: 0.4784\n",
      "Epoch 41/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.4783 - acc: 0.4622 - val_loss: 1.2871 - val_acc: 0.5229\n",
      "Epoch 42/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.4897 - acc: 0.4646 - val_loss: 1.5929 - val_acc: 0.4226\n",
      "Epoch 43/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 1.4734 - acc: 0.4693 - val_loss: 2.0095 - val_acc: 0.3556\n",
      "Epoch 44/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.4654 - acc: 0.4685 - val_loss: 1.2940 - val_acc: 0.5141\n",
      "Epoch 45/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 1.4510 - acc: 0.4726 - val_loss: 1.5039 - val_acc: 0.4680\n",
      "Epoch 46/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.4464 - acc: 0.4772 - val_loss: 2.5661 - val_acc: 0.2523\n",
      "Epoch 47/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.4447 - acc: 0.4788 - val_loss: 1.5765 - val_acc: 0.4100\n",
      "Epoch 48/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.4412 - acc: 0.4837 - val_loss: 1.5835 - val_acc: 0.4363\n",
      "Epoch 49/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.4374 - acc: 0.4854 - val_loss: 1.8157 - val_acc: 0.3844\n",
      "Epoch 50/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.4292 - acc: 0.4834 - val_loss: 1.3497 - val_acc: 0.5077\n",
      "Epoch 51/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 1.4060 - acc: 0.4913 - val_loss: 1.2849 - val_acc: 0.5201\n",
      "Epoch 52/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 1.4015 - acc: 0.4968 - val_loss: 1.4019 - val_acc: 0.4709\n",
      "Epoch 53/100\n",
      "20455/20455 [==============================] - 1s 45us/step - loss: 1.4032 - acc: 0.4932 - val_loss: 1.4552 - val_acc: 0.4753\n",
      "Epoch 54/100\n",
      "20455/20455 [==============================] - 1s 45us/step - loss: 1.4167 - acc: 0.4918 - val_loss: 2.5014 - val_acc: 0.3623\n",
      "Epoch 55/100\n",
      "20455/20455 [==============================] - 1s 45us/step - loss: 1.3921 - acc: 0.5011 - val_loss: 2.2698 - val_acc: 0.3240\n",
      "Epoch 56/100\n",
      "20455/20455 [==============================] - 1s 44us/step - loss: 1.3929 - acc: 0.5032 - val_loss: 1.7299 - val_acc: 0.4186\n",
      "Epoch 57/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.3908 - acc: 0.5042 - val_loss: 1.2031 - val_acc: 0.5483\n",
      "Epoch 58/100\n",
      "20455/20455 [==============================] - 1s 37us/step - loss: 1.3774 - acc: 0.4969 - val_loss: 1.2134 - val_acc: 0.5444\n",
      "Epoch 59/100\n",
      "20455/20455 [==============================] - 1s 35us/step - loss: 1.3745 - acc: 0.5053 - val_loss: 1.8772 - val_acc: 0.4280\n",
      "Epoch 60/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.3711 - acc: 0.5058 - val_loss: 1.1896 - val_acc: 0.5539\n",
      "Epoch 61/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3581 - acc: 0.5066 - val_loss: 1.7396 - val_acc: 0.4247\n",
      "Epoch 62/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3751 - acc: 0.5095 - val_loss: 1.5199 - val_acc: 0.4480\n",
      "Epoch 63/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3643 - acc: 0.5147 - val_loss: 1.1487 - val_acc: 0.5783\n",
      "Epoch 64/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3416 - acc: 0.5198 - val_loss: 1.5403 - val_acc: 0.4479\n",
      "Epoch 65/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3595 - acc: 0.5125 - val_loss: 2.4316 - val_acc: 0.3257\n",
      "Epoch 66/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3465 - acc: 0.5154 - val_loss: 1.4661 - val_acc: 0.4423\n",
      "Epoch 67/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.3519 - acc: 0.5139 - val_loss: 1.3640 - val_acc: 0.5011\n",
      "Epoch 68/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3277 - acc: 0.5218 - val_loss: 1.3989 - val_acc: 0.4986\n",
      "Epoch 69/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3348 - acc: 0.5215 - val_loss: 1.1714 - val_acc: 0.5650\n",
      "Epoch 70/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3384 - acc: 0.5193 - val_loss: 1.2406 - val_acc: 0.5326\n",
      "Epoch 71/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3155 - acc: 0.5225 - val_loss: 1.3177 - val_acc: 0.5103\n",
      "Epoch 72/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3325 - acc: 0.5218 - val_loss: 1.1204 - val_acc: 0.5810\n",
      "Epoch 73/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.3058 - acc: 0.5278 - val_loss: 1.1521 - val_acc: 0.5650\n",
      "Epoch 74/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3130 - acc: 0.5326 - val_loss: 1.2621 - val_acc: 0.5067\n",
      "Epoch 75/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3089 - acc: 0.5282 - val_loss: 1.0944 - val_acc: 0.5916\n",
      "Epoch 76/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3109 - acc: 0.5326 - val_loss: 1.1616 - val_acc: 0.5620\n",
      "Epoch 77/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.2930 - acc: 0.5300 - val_loss: 1.4157 - val_acc: 0.4800\n",
      "Epoch 78/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3037 - acc: 0.5305 - val_loss: 1.0846 - val_acc: 0.6007\n",
      "Epoch 79/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.3188 - acc: 0.5261 - val_loss: 1.1274 - val_acc: 0.5837\n",
      "Epoch 80/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.2749 - acc: 0.5366 - val_loss: 1.6470 - val_acc: 0.4111\n",
      "Epoch 81/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.2982 - acc: 0.5312 - val_loss: 1.1691 - val_acc: 0.5703\n",
      "Epoch 82/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.2953 - acc: 0.5298 - val_loss: 1.6460 - val_acc: 0.4333\n",
      "Epoch 83/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.2889 - acc: 0.5321 - val_loss: 1.1208 - val_acc: 0.5764\n",
      "Epoch 84/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.2854 - acc: 0.5353 - val_loss: 1.0872 - val_acc: 0.6020\n",
      "Epoch 85/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.2846 - acc: 0.5386 - val_loss: 1.8823 - val_acc: 0.3977\n",
      "Epoch 86/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.2952 - acc: 0.5382 - val_loss: 1.1679 - val_acc: 0.5636\n",
      "Epoch 87/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.2547 - acc: 0.5457 - val_loss: 1.1554 - val_acc: 0.5584\n",
      "Epoch 88/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.2650 - acc: 0.5441 - val_loss: 1.2172 - val_acc: 0.5406\n",
      "Epoch 89/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.2726 - acc: 0.5410 - val_loss: 1.0766 - val_acc: 0.6046\n",
      "Epoch 90/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.2610 - acc: 0.5469 - val_loss: 1.4181 - val_acc: 0.4937\n",
      "Epoch 91/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.2809 - acc: 0.5440 - val_loss: 1.1189 - val_acc: 0.5883\n",
      "Epoch 92/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.2826 - acc: 0.5475 - val_loss: 1.1628 - val_acc: 0.5647\n",
      "Epoch 93/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.2477 - acc: 0.5477 - val_loss: 1.2848 - val_acc: 0.5227\n",
      "Epoch 94/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.2493 - acc: 0.5515 - val_loss: 1.2791 - val_acc: 0.5237\n",
      "Epoch 95/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.2569 - acc: 0.5481 - val_loss: 1.9314 - val_acc: 0.3927\n",
      "Epoch 96/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.2555 - acc: 0.5501 - val_loss: 1.1176 - val_acc: 0.5830\n",
      "Epoch 97/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.2511 - acc: 0.5527 - val_loss: 1.1805 - val_acc: 0.5574\n",
      "Epoch 98/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.2455 - acc: 0.5505 - val_loss: 1.0323 - val_acc: 0.6066\n",
      "Epoch 99/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.2339 - acc: 0.5516 - val_loss: 1.3986 - val_acc: 0.4867\n",
      "Epoch 100/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.2458 - acc: 0.5501 - val_loss: 1.3179 - val_acc: 0.5247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11f273e80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Creación de red secuencial\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dense(30, init='uniform', activation='relu'))\n",
    "model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr.values, to_categorical(y_tr), nb_epoch=100, batch_size=128, verbose=1,\n",
    "validation_data=(x_v.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  1 10 ...,  2  4  2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3930563301728946"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_t_pred = model.predict_classes(x_t.values)\n",
    "accuracy_score(y_t, y_t_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Para la mejor red entrenada anteriormente construya la matriz de confusi´on de las distintas clases, para\n",
    "asi visualizar cu´ales son las clases m´as dif´ıciles de clasificar y con cu´ales se confunden. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAywAAANHCAYAAADUvdkEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYAEAAKYlsAAAANMSWAAAgGm9AeQqGPywKjKHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114af1a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_t_predict = model.predict_classes(x_t.values)\n",
    "cm = confusion_matrix(y_t, y_t_predict)\n",
    "    \n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Entrene una SVM no lineal sobre los pixeles con y sin pre-procesamiento. Puede utilizar el conjunto de\n",
    "validaci´on para seleccionar hiper-par´ametros, como el nivel de regularizaci´on aplicado y/o la funci´on\n",
    "de kernel a utilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sin procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying param 0.001000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC as SVM\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_svm(param):\n",
    "    model = SVM()\n",
    "    model.set_params(C=param, kernel='rbf')\n",
    "    model.fit(x_tr, y_tr)\n",
    "\n",
    "    y_tr_pred = model.predict(x_tr)\n",
    "    y_v_pred = model.predict(x_v)\n",
    "\n",
    "    train_error = (1-accuracy_score(y_tr, y_tr_pred))\n",
    "    test_error = (1-accuracy_score(y_v, y_v_pred))\n",
    "    \n",
    "    return (train_error, test_error)\n",
    "\n",
    "def graph_svm_range(params):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for depth in params:\n",
    "        print('Trying param %f' % depth)\n",
    "        (train, test) = train_svm(depth)\n",
    "        train_errors.append(train)\n",
    "        test_errors.append(test)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(params, train_errors, label=\"Train Error\")\n",
    "    plt.plot(params, test_errors, label=\"Test Error\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('RBF')\n",
    "    plt.ylabel('Error')\n",
    "    plt.show()\n",
    "    \n",
    "params = np.arange(0.001, 1.0, 0.25)\n",
    "graph_svm_range(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Con procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** Entrene una ´arbol de clasificaci´on sobre los pixeles con y sin pre-procesamiento. Puede utilizar el\n",
    "conjunto de validaci´on para seleccionar hiper-par´ametros, como la profundidad m´axima del ´arbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_tree(depth):\n",
    "    # Entrenar el árbol\n",
    "    model = Tree()\n",
    "    model.set_params(max_depth=depth, criterion='gini', splitter='best')\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "\n",
    "    y_tr_pred = model.predict(x_tr)\n",
    "    y_v_pred = model.predict(x_v)\n",
    "\n",
    "\n",
    "    train_error = (1-accuracy_score(y_tr, Y_tr_pred))\n",
    "    test_error = (1-accuracy_score(y_v, y_v_pred))\n",
    "\n",
    "    return (train_error, test_error)\n",
    "\n",
    "def graph_tree_range(params):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for depth in params:\n",
    "        print('Trying depth %d' % depth)\n",
    "        (train, test) = train_tree(depth)\n",
    "        train_errors.append(train)\n",
    "        test_errors.append(test)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(params, train_errors, label=\"Train Error\")\n",
    "    plt.plot(params, test_errors, label=\"Test Error\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('Profundidad del árbol')\n",
    "    plt.ylabel('Error')\n",
    "    plt.show()\n",
    "    \n",
    "params = np.arange(1, 20, 1)\n",
    "graph_tree_range(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
