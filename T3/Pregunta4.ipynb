{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Construya una funci´on que cargue todos los datos de entrenamiento y pruebas del problema generando\n",
    "como salida: (i) dos matrices Xtr, Ytr, correspondientes a las im´agenes y etiquetas de entrenamiento,\n",
    "(ii) dos matrices Xt, Yt, correspondientes a las im´agenes y etiquetas de pruebas, y finalmente (iii) dos\n",
    "matrices Xv, Yv, correspondientes a im´agenes y etiquetas que se usar´an como conjunto de validaci´on, es\n",
    "decir para tomar decisiones de dise˜no acerca del modelo. Este ´ultimo conjunto debe ser extra´ıdo desde\n",
    "el conjunto de entrenamiento original y no debe superar las 7000 im´agenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "def load_data():\n",
    "    train = pd.read_csv('data/sign_mnist_train.csv')\n",
    "    test = pd.read_csv('data/sign_mnist_test.csv')\n",
    "    (train_set, validation_set) = train_test_split(train, test_size=7000, random_state=8500)\n",
    "    \n",
    "    y_tr = train_set['label']\n",
    "    x_tr = train_set.iloc[:, 1:]\n",
    "    \n",
    "    y_t = test['label']\n",
    "    x_t = test.iloc[:, 1:]\n",
    "    \n",
    "    y_v = validation_set['label']\n",
    "    x_v = validation_set.iloc[:, 1:]\n",
    "\n",
    "    return(x_tr,x_v,x_t,y_tr,y_v,y_t)\n",
    "x_tr, x_v, x_t, y_tr, y_v , y_t = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Construya una funci´on que escale apropiadamente las im´agenes antes de trabajar. Experimente s´olo\n",
    "escalando los datos de acuerdo a la intensidad m´axima de pixel (i.e., dividiendo por 255) y luego\n",
    "centrando y escal´andolos como en actividades anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_by_max_pixel(x_set):\n",
    "    return x_set/255\n",
    "\n",
    "\n",
    "# Finish this one!\n",
    "def center_and_scale(x_set):\n",
    "    return x_set\n",
    "x_tr = scale_by_max_pixel(x_tr)\n",
    "x_t = scale_by_max_pixel(x_t)\n",
    "x_v = scale_by_max_pixel(x_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Dise˜ne, entrene y eval´ue una red neuronal para el problema partir de la representaci´on original de las\n",
    "im´agenes. Experimente con distintas arquitecturas, pre-procesamientos y m´etodos de entrenamiento, midiendo el error de clasificaci´on sobre el conjunto de validaci´on. En base a esta ´ultima medida de\n",
    "desempe˜no, decida qu´e modelo, de entre todos los evaluados, medir´a finalmente en el conjunto de test.\n",
    "Reporte y discuta los resultados obtenidos. Se espera que logre obtener un error de pruebas menor o\n",
    "igual a 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20455 samples, validate on 7000 samples\n",
      "Epoch 1/100\n",
      "20455/20455 [==============================] - 1s 56us/step - loss: 3.0948 - acc: 0.0777 - val_loss: 2.8797 - val_acc: 0.1133\n",
      "Epoch 2/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 2.7927 - acc: 0.1252 - val_loss: 2.7313 - val_acc: 0.1384\n",
      "Epoch 3/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 2.6680 - acc: 0.1569 - val_loss: 2.5864 - val_acc: 0.1934\n",
      "Epoch 4/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 2.4733 - acc: 0.2086 - val_loss: 2.3807 - val_acc: 0.2271\n",
      "Epoch 5/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 2.3196 - acc: 0.2376 - val_loss: 2.2874 - val_acc: 0.2571\n",
      "Epoch 6/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 2.2344 - acc: 0.2579 - val_loss: 2.2137 - val_acc: 0.2714\n",
      "Epoch 7/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 2.1752 - acc: 0.2752 - val_loss: 2.1532 - val_acc: 0.2784\n",
      "Epoch 8/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 2.1099 - acc: 0.2985 - val_loss: 2.0897 - val_acc: 0.3189\n",
      "Epoch 9/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 2.0331 - acc: 0.3283 - val_loss: 2.0003 - val_acc: 0.3434\n",
      "Epoch 10/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.9424 - acc: 0.3616 - val_loss: 1.9061 - val_acc: 0.3711\n",
      "Epoch 11/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.8565 - acc: 0.3920 - val_loss: 1.8110 - val_acc: 0.4281\n",
      "Epoch 12/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.7793 - acc: 0.4179 - val_loss: 1.7670 - val_acc: 0.4317\n",
      "Epoch 13/100\n",
      "20455/20455 [==============================] - 1s 44us/step - loss: 1.7159 - acc: 0.4357 - val_loss: 1.7241 - val_acc: 0.4249\n",
      "Epoch 14/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.6671 - acc: 0.4534 - val_loss: 1.7645 - val_acc: 0.4280\n",
      "Epoch 15/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.6207 - acc: 0.4654 - val_loss: 1.6059 - val_acc: 0.4804\n",
      "Epoch 16/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.5872 - acc: 0.4724 - val_loss: 1.6512 - val_acc: 0.4667\n",
      "Epoch 17/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.5535 - acc: 0.4808 - val_loss: 1.5909 - val_acc: 0.4797\n",
      "Epoch 18/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.5200 - acc: 0.4962 - val_loss: 1.6358 - val_acc: 0.4593\n",
      "Epoch 19/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.4902 - acc: 0.4978 - val_loss: 1.5564 - val_acc: 0.4964\n",
      "Epoch 20/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 1.4660 - acc: 0.5109 - val_loss: 1.4610 - val_acc: 0.5179\n",
      "Epoch 21/100\n",
      "20455/20455 [==============================] - 1s 44us/step - loss: 1.4333 - acc: 0.5215 - val_loss: 1.4462 - val_acc: 0.5221\n",
      "Epoch 22/100\n",
      "20455/20455 [==============================] - 1s 47us/step - loss: 1.4083 - acc: 0.5295 - val_loss: 1.4896 - val_acc: 0.5056\n",
      "Epoch 23/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 1.3801 - acc: 0.5360 - val_loss: 1.3798 - val_acc: 0.5341\n",
      "Epoch 24/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 1.3487 - acc: 0.5498 - val_loss: 1.3448 - val_acc: 0.5470\n",
      "Epoch 25/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 1.3248 - acc: 0.5540 - val_loss: 1.3564 - val_acc: 0.5381\n",
      "Epoch 26/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 1.2975 - acc: 0.5654 - val_loss: 1.3112 - val_acc: 0.5631\n",
      "Epoch 27/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.2702 - acc: 0.5730 - val_loss: 1.2941 - val_acc: 0.5733\n",
      "Epoch 28/100\n",
      "20455/20455 [==============================] - 1s 48us/step - loss: 1.2363 - acc: 0.5839 - val_loss: 1.2270 - val_acc: 0.5959\n",
      "Epoch 29/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 1.2104 - acc: 0.5953 - val_loss: 1.2403 - val_acc: 0.5913\n",
      "Epoch 30/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 1.1781 - acc: 0.5999 - val_loss: 1.3388 - val_acc: 0.5434\n",
      "Epoch 31/100\n",
      "20455/20455 [==============================] - 1s 46us/step - loss: 1.1484 - acc: 0.6131 - val_loss: 1.1568 - val_acc: 0.6040\n",
      "Epoch 32/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 1.1142 - acc: 0.6270 - val_loss: 1.1686 - val_acc: 0.6131\n",
      "Epoch 33/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.0855 - acc: 0.6344 - val_loss: 1.0643 - val_acc: 0.6403\n",
      "Epoch 34/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 1.0560 - acc: 0.6435 - val_loss: 1.0835 - val_acc: 0.6321\n",
      "Epoch 35/100\n",
      "20455/20455 [==============================] - 1s 47us/step - loss: 1.0264 - acc: 0.6554 - val_loss: 1.1132 - val_acc: 0.6211\n",
      "Epoch 36/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 1.0005 - acc: 0.6626 - val_loss: 1.1160 - val_acc: 0.6121\n",
      "Epoch 37/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 0.9745 - acc: 0.6690 - val_loss: 1.0267 - val_acc: 0.6510\n",
      "Epoch 38/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 0.9506 - acc: 0.6767 - val_loss: 1.0902 - val_acc: 0.6314\n",
      "Epoch 39/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 0.9264 - acc: 0.6830 - val_loss: 0.9197 - val_acc: 0.6854\n",
      "Epoch 40/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 0.8995 - acc: 0.6966 - val_loss: 0.9398 - val_acc: 0.6740\n",
      "Epoch 41/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 0.8821 - acc: 0.6984 - val_loss: 0.9163 - val_acc: 0.6796\n",
      "Epoch 42/100\n",
      "20455/20455 [==============================] - 1s 38us/step - loss: 0.8555 - acc: 0.7098 - val_loss: 0.8858 - val_acc: 0.6994\n",
      "Epoch 43/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 0.8320 - acc: 0.7181 - val_loss: 0.8538 - val_acc: 0.7157\n",
      "Epoch 44/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 0.8086 - acc: 0.7251 - val_loss: 0.8221 - val_acc: 0.7217\n",
      "Epoch 45/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 0.7868 - acc: 0.7339 - val_loss: 0.9947 - val_acc: 0.6561\n",
      "Epoch 46/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 0.7609 - acc: 0.7434 - val_loss: 0.7367 - val_acc: 0.7607\n",
      "Epoch 47/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 0.7366 - acc: 0.7519 - val_loss: 0.7603 - val_acc: 0.7356\n",
      "Epoch 48/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 0.7185 - acc: 0.7559 - val_loss: 0.6807 - val_acc: 0.7787\n",
      "Epoch 49/100\n",
      "20455/20455 [==============================] - 1s 38us/step - loss: 0.6928 - acc: 0.7663 - val_loss: 0.9222 - val_acc: 0.6851\n",
      "Epoch 50/100\n",
      "20455/20455 [==============================] - 1s 45us/step - loss: 0.6771 - acc: 0.7717 - val_loss: 0.7177 - val_acc: 0.7481\n",
      "Epoch 51/100\n",
      "20455/20455 [==============================] - 1s 38us/step - loss: 0.6587 - acc: 0.7747 - val_loss: 0.6581 - val_acc: 0.7791\n",
      "Epoch 52/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 0.6404 - acc: 0.7848 - val_loss: 0.7308 - val_acc: 0.7433\n",
      "Epoch 53/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 0.6237 - acc: 0.7904 - val_loss: 0.6391 - val_acc: 0.7864\n",
      "Epoch 54/100\n",
      "20455/20455 [==============================] - 1s 45us/step - loss: 0.6066 - acc: 0.7949 - val_loss: 0.5937 - val_acc: 0.8124\n",
      "Epoch 55/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 0.5886 - acc: 0.8021 - val_loss: 0.6544 - val_acc: 0.7637\n",
      "Epoch 56/100\n",
      "20455/20455 [==============================] - 1s 44us/step - loss: 0.5765 - acc: 0.8055 - val_loss: 0.5499 - val_acc: 0.8244\n",
      "Epoch 57/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 0.5613 - acc: 0.8089 - val_loss: 0.5397 - val_acc: 0.8126\n",
      "Epoch 58/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 0.5438 - acc: 0.8150 - val_loss: 0.6361 - val_acc: 0.7674\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20455/20455 [==============================] - 1s 41us/step - loss: 0.5310 - acc: 0.8212 - val_loss: 0.5325 - val_acc: 0.8156\n",
      "Epoch 60/100\n",
      "20455/20455 [==============================] - 1s 45us/step - loss: 0.5158 - acc: 0.8251 - val_loss: 0.6677 - val_acc: 0.7560\n",
      "Epoch 61/100\n",
      "20455/20455 [==============================] - 1s 44us/step - loss: 0.5000 - acc: 0.8329 - val_loss: 0.4926 - val_acc: 0.8401\n",
      "Epoch 62/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 0.4917 - acc: 0.8327 - val_loss: 0.4671 - val_acc: 0.8479\n",
      "Epoch 63/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 0.4789 - acc: 0.8364 - val_loss: 0.4633 - val_acc: 0.8447\n",
      "Epoch 64/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 0.4674 - acc: 0.8390 - val_loss: 0.5168 - val_acc: 0.8250\n",
      "Epoch 65/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 0.4534 - acc: 0.8458 - val_loss: 0.5164 - val_acc: 0.8253\n",
      "Epoch 66/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 0.4437 - acc: 0.8479 - val_loss: 0.5305 - val_acc: 0.8061\n",
      "Epoch 67/100\n",
      "20455/20455 [==============================] - 1s 44us/step - loss: 0.4298 - acc: 0.8551 - val_loss: 0.4812 - val_acc: 0.8320\n",
      "Epoch 68/100\n",
      "20455/20455 [==============================] - 1s 45us/step - loss: 0.4228 - acc: 0.8552 - val_loss: 0.4668 - val_acc: 0.8280\n",
      "Epoch 69/100\n",
      "20455/20455 [==============================] - 1s 44us/step - loss: 0.4098 - acc: 0.8583 - val_loss: 0.3730 - val_acc: 0.8787\n",
      "Epoch 70/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 0.4022 - acc: 0.8612 - val_loss: 0.3815 - val_acc: 0.8666\n",
      "Epoch 71/100\n",
      "20455/20455 [==============================] - 1s 44us/step - loss: 0.3921 - acc: 0.8651 - val_loss: 0.3679 - val_acc: 0.8753\n",
      "Epoch 72/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 0.3813 - acc: 0.8661 - val_loss: 0.3891 - val_acc: 0.8644\n",
      "Epoch 73/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 0.3706 - acc: 0.8739 - val_loss: 0.4486 - val_acc: 0.8314\n",
      "Epoch 74/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 0.3564 - acc: 0.8801 - val_loss: 0.3942 - val_acc: 0.8646\n",
      "Epoch 75/100\n",
      "20455/20455 [==============================] - 1s 46us/step - loss: 0.3530 - acc: 0.8814 - val_loss: 0.3283 - val_acc: 0.8913\n",
      "Epoch 76/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 0.3454 - acc: 0.8792 - val_loss: 0.3822 - val_acc: 0.8680\n",
      "Epoch 77/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 0.3348 - acc: 0.8879 - val_loss: 0.2933 - val_acc: 0.9054\n",
      "Epoch 78/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 0.3232 - acc: 0.8921 - val_loss: 0.4993 - val_acc: 0.8160\n",
      "Epoch 79/100\n",
      "20455/20455 [==============================] - 1s 39us/step - loss: 0.3191 - acc: 0.8935 - val_loss: 0.3083 - val_acc: 0.8959\n",
      "Epoch 80/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 0.3138 - acc: 0.8937 - val_loss: 0.3068 - val_acc: 0.8873\n",
      "Epoch 81/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 0.3003 - acc: 0.8957 - val_loss: 0.3464 - val_acc: 0.8746\n",
      "Epoch 82/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 0.2900 - acc: 0.8999 - val_loss: 0.2424 - val_acc: 0.9246\n",
      "Epoch 83/100\n",
      "20455/20455 [==============================] - 1s 40us/step - loss: 0.2868 - acc: 0.9033 - val_loss: 0.2303 - val_acc: 0.9307\n",
      "Epoch 84/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 0.2794 - acc: 0.9065 - val_loss: 0.3292 - val_acc: 0.8874\n",
      "Epoch 85/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 0.2737 - acc: 0.9064 - val_loss: 0.2374 - val_acc: 0.9206\n",
      "Epoch 86/100\n",
      "20455/20455 [==============================] - 1s 50us/step - loss: 0.2647 - acc: 0.9122 - val_loss: 0.2826 - val_acc: 0.8954\n",
      "Epoch 87/100\n",
      "20455/20455 [==============================] - 1s 46us/step - loss: 0.2601 - acc: 0.9133 - val_loss: 0.3070 - val_acc: 0.8917\n",
      "Epoch 88/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 0.2479 - acc: 0.9173 - val_loss: 0.2496 - val_acc: 0.9104\n",
      "Epoch 89/100\n",
      "20455/20455 [==============================] - 1s 45us/step - loss: 0.2550 - acc: 0.9154 - val_loss: 0.2333 - val_acc: 0.9229\n",
      "Epoch 90/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 0.2435 - acc: 0.9175 - val_loss: 0.1887 - val_acc: 0.9486\n",
      "Epoch 91/100\n",
      "20455/20455 [==============================] - 1s 45us/step - loss: 0.2280 - acc: 0.9248 - val_loss: 0.1857 - val_acc: 0.9470\n",
      "Epoch 92/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 0.2318 - acc: 0.9254 - val_loss: 0.4310 - val_acc: 0.8413\n",
      "Epoch 93/100\n",
      "20455/20455 [==============================] - 1s 44us/step - loss: 0.2254 - acc: 0.9271 - val_loss: 0.1695 - val_acc: 0.9556\n",
      "Epoch 94/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 0.2157 - acc: 0.9296 - val_loss: 0.3509 - val_acc: 0.8719\n",
      "Epoch 95/100\n",
      "20455/20455 [==============================] - 1s 44us/step - loss: 0.2171 - acc: 0.9312 - val_loss: 0.2620 - val_acc: 0.9066\n",
      "Epoch 96/100\n",
      "20455/20455 [==============================] - 1s 46us/step - loss: 0.1985 - acc: 0.9364 - val_loss: 0.4391 - val_acc: 0.8473\n",
      "Epoch 97/100\n",
      "20455/20455 [==============================] - 1s 42us/step - loss: 0.2037 - acc: 0.9352 - val_loss: 0.2925 - val_acc: 0.8881\n",
      "Epoch 98/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 0.2031 - acc: 0.9353 - val_loss: 0.2665 - val_acc: 0.9029\n",
      "Epoch 99/100\n",
      "20455/20455 [==============================] - 1s 43us/step - loss: 0.1982 - acc: 0.9377 - val_loss: 0.1968 - val_acc: 0.9353\n",
      "Epoch 100/100\n",
      "20455/20455 [==============================] - 1s 41us/step - loss: 0.1983 - acc: 0.9375 - val_loss: 0.1203 - val_acc: 0.9739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1249b4080>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Creación de red secuencial\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dense(30, init='uniform', activation='relu'))\n",
    "model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr.values, to_categorical(y_tr), nb_epoch=100, batch_size=128, verbose=1,\n",
    "validation_data=(x_v.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   3.32073126e-28   4.67418395e-05 ...,   7.46003329e-11\n",
      "    5.98884897e-09   5.29813216e-12]\n",
      " [  2.03693594e-15   2.07727298e-01   1.32686917e-10 ...,   1.88328996e-02\n",
      "    1.08460430e-03   3.81185295e-04]\n",
      " [  4.98152877e-31   1.89220931e-10   4.30337736e-18 ...,   1.77908601e-06\n",
      "    6.94260294e-08   2.06336088e-04]\n",
      " ..., \n",
      " [  0.00000000e+00   0.00000000e+00   9.99354184e-01 ...,   1.12837481e-25\n",
      "    2.87218469e-12   1.83773519e-31]\n",
      " [  2.86806039e-12   5.67856906e-20   4.34892465e-15 ...,   1.47701203e-32\n",
      "    3.09394434e-17   4.15056241e-31]\n",
      " [  0.00000000e+00   0.00000000e+00   9.99555647e-01 ...,   1.73227353e-26\n",
      "    1.60276274e-12   3.30326961e-32]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_t_pred = model.predict(x_t)\n",
    "\n",
    "print(y_t_pred)\n",
    "#accuracy_score(y_t, y_t_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Para la mejor red entrenada anteriormente construya la matriz de confusi´on de las distintas clases, para\n",
    "asi visualizar cu´ales son las clases m´as dif´ıciles de clasificar y con cu´ales se confunden. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAywAAANHCAYAAADUvdkEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121b039b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_t_predict = model.predict_classes(x_t.values)\n",
    "cm = confusion_matrix(y_t, y_t_predict)\n",
    "    \n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Entrene una SVM no lineal sobre los pixeles con y sin pre-procesamiento. Puede utilizar el conjunto de\n",
    "validaci´on para seleccionar hiper-par´ametros, como el nivel de regularizaci´on aplicado y/o la funci´on\n",
    "de kernel a utilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sin procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying param 0.001000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC as SVM\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_svm(param):\n",
    "    model = SVM()\n",
    "    model.set_params(C=param, kernel='rbf')\n",
    "    model.fit(x_tr, y_tr)\n",
    "\n",
    "    y_tr_pred = model.predict(x_tr)\n",
    "    y_v_pred = model.predict(x_v)\n",
    "\n",
    "    train_error = (1-accuracy_score(y_tr, y_tr_pred))\n",
    "    test_error = (1-accuracy_score(y_v, y_v_pred))\n",
    "    \n",
    "    return (train_error, test_error)\n",
    "\n",
    "def graph_svm_range(params):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for depth in params:\n",
    "        print('Trying param %f' % depth)\n",
    "        (train, test) = train_svm(depth)\n",
    "        train_errors.append(train)\n",
    "        test_errors.append(test)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(params, train_errors, label=\"Train Error\")\n",
    "    plt.plot(params, test_errors, label=\"Test Error\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('RBF')\n",
    "    plt.ylabel('Error')\n",
    "    plt.show()\n",
    "    \n",
    "params = np.arange(0.001, 1.0, 0.05)\n",
    "graph_svm_range(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Con procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** Entrene una ´arbol de clasificaci´on sobre los pixeles con y sin pre-procesamiento. Puede utilizar el\n",
    "conjunto de validaci´on para seleccionar hiper-par´ametros, como la profundidad m´axima del ´arbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_tree(depth):\n",
    "    # Entrenar el árbol\n",
    "    model = Tree()\n",
    "    model.set_params(max_depth=depth, criterion='gini', splitter='best')\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "\n",
    "    y_tr_pred = model.predict(x_tr)\n",
    "    y_v_pred = model.predict(x_v)\n",
    "\n",
    "\n",
    "    train_error = (1-accuracy_score(y_tr, Y_tr_pred))\n",
    "    test_error = (1-accuracy_score(y_v, y_v_pred))\n",
    "\n",
    "    return (train_error, test_error)\n",
    "\n",
    "def graph_tree_range(params):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for depth in params:\n",
    "        print('Trying depth %d' % depth)\n",
    "        (train, test) = train_tree(depth)\n",
    "        train_errors.append(train)\n",
    "        test_errors.append(test)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(params, train_errors, label=\"Train Error\")\n",
    "    plt.plot(params, test_errors, label=\"Test Error\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('Profundidad del árbol')\n",
    "    plt.ylabel('Error')\n",
    "    plt.show()\n",
    "    \n",
    "params = np.arange(1, 20, 1)\n",
    "graph_tree_range(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
